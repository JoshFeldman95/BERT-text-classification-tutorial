{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cjp7nhbQpP6N",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "to run presentation, enter `jupyter nbconvert Tutorial.ipynb --to slides --post serve` in the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_iLp3cYpP6P",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Classification with BERT\n",
    "\n",
    "<img src=\"img/bert_reading.jpg\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "Tm36IqTVpS65",
    "outputId": "c9b83958-a366-4234-f04d-60f396432f20",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.13.2)\n",
      "Requirement already satisfied: params-flow>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.7.4)\n",
      "Requirement already satisfied: py-params>=0.7.3 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "1mjIEgrspP6Q",
    "outputId": "8661f46e-ac42-4c89-b147-bc4693fae686",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9x3Iur-OpP6S",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop Goal in Machine Learning Jargon:\n",
    "\n",
    "_Leverage a **bidirectional language model** with a **transformer architecture** for **text classification** using **transfer learning**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "muuG6ARgpP6T",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Right now, this should make no sense!\n",
    "\n",
    "But by the end of this workshop, you will not only understand what this means, but you will be able to use this method to train a natural language processing model with only 500 data points.\n",
    "\n",
    "**Note:** This code in this tutorial is largely based on Google's tutorial available [here](https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb) and this Towards Data Science post available [here](https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ra1bEW5pP6T"
   },
   "source": [
    "# The Road Ahead\n",
    "\n",
    "**Goal:** _Leverage a **bidirectional language model** with a **transformer architecture** for **text classification** using **transfer learning**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1Tv9pHhpP6U"
   },
   "source": [
    "1. Text Classification\n",
    "2. Transfer Learning\n",
    "3. Bidirectional Language Model\n",
    "4. Transformer Architecture\n",
    "5. Put it all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qPNUP2fpP6V"
   },
   "source": [
    "# 1. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "716p8B1zpP6V"
   },
   "source": [
    "Text classification is... classifying text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sn8LZlDZpP6W"
   },
   "source": [
    "### Sentiment analysis:\n",
    "<img src=\"img/harvard_review.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qip68_OpP6W"
   },
   "source": [
    "### Topic classification\n",
    "<img src=\"img/topic_classification.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MS3to4xSpP6X"
   },
   "source": [
    "## IMDb Sentiment Analysis\n",
    "\n",
    "Today, we're going to be working with the IMDb sentiment analysis dataset.\n",
    "\n",
    "**The Task:** Classify a movie review on IMDb as <span style=\"color:green\">positive</span> or <span style=\"color:red\">negative</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "if5TuYMXpP6X"
   },
   "source": [
    "## Loading the Data\n",
    "\n",
    "We will be using the IMDB Large Movie Review Dataset, which is hosted by Stanford."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWFjUEGUpP6Y"
   },
   "source": [
    "**TODO: Look into hosting data ourselves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EteIRYqfpP6Y"
   },
   "outputs": [],
   "source": [
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in tqdm(os.listdir(directory), desc=os.path.basename(directory)):\n",
    "        with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBtRY-2OpP6a"
   },
   "source": [
    "We now define a class that will handle downloading and preparing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PkVnWMZEpP6b"
   },
   "source": [
    "```python\n",
    "class MovieReviewData:\n",
    "    DATA_COLUMN = \"sentence\"\n",
    "    LABEL_COLUMN = \"polarity\"\n",
    "\n",
    "    def __init__(self, tokenizer: FullTokenizer, sample_size=None, max_seq_len=128):\n",
    "        # Download the data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_size = sample_size\n",
    "        self.max_seq_len = 0\n",
    "        train, test = download_and_load_datasets()\n",
    "        \n",
    "        # Re-index by length of text\n",
    "        train, test = map(lambda df: \n",
    "                          df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index), \n",
    "                          [train, test])\n",
    "        \n",
    "        # sample the data\n",
    "        if sample_size is not None:\n",
    "            assert sample_size % 128 == 0\n",
    "            train, test = train.head(sample_size), test.head(sample_size)\n",
    "            # train, test = map(lambda df: df.sample(sample_size), [train, test])\n",
    "        \n",
    "        # prepare the data\n",
    "        ((self.train_x, self.train_y),\n",
    "         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "        \n",
    "        # pad the data as needed\n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        ((self.train_x, self.train_x_token_types),\n",
    "         (self.test_x, self.test_x_token_types)) = map(self._pad, \n",
    "                                                       [self.train_x, self.test_x])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70OacBWgpP6e"
   },
   "source": [
    "```python\n",
    "def _prepare(self, df):\n",
    "    x, y = [], []\n",
    "    with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
    "        for ndx, row in df.iterrows():\n",
    "            # get text and label from row of data\n",
    "            text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]\n",
    "\n",
    "            # tokenize\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "            # append special start and end tokens\n",
    "            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "\n",
    "            # convert tokens to IDs\n",
    "            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            # clip texts to keep it under the max sequence length threshold\n",
    "            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "\n",
    "            # save results\n",
    "            x.append(token_ids)\n",
    "            y.append(int(label))\n",
    "            pbar.update()\n",
    "    return np.array(x), np.array(y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwNolX47pP6f"
   },
   "source": [
    "```python\n",
    "def _pad(self, ids):\n",
    "        # The function pads the texts if they are less than the max sequence length threshold\n",
    "        x, t = [], []\n",
    "        token_type_ids = [0] * self.max_seq_len\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "            t.append(token_type_ids)\n",
    "        return np.array(x), np.array(t)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qc-jNuYapP6h"
   },
   "outputs": [],
   "source": [
    "class MovieReviewData:\n",
    "    DATA_COLUMN = \"sentence\"\n",
    "    LABEL_COLUMN = \"polarity\"\n",
    "\n",
    "    def __init__(self, tokenizer, sample_size=None, max_seq_len=128):\n",
    "        # Download the data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_size = sample_size\n",
    "        self.max_seq_len = 0\n",
    "        train, test = download_and_load_datasets()\n",
    "        \n",
    "        # Re-index by length of text\n",
    "        train, test = map(lambda df: df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index), \n",
    "                          [train, test])\n",
    "        \n",
    "        # sample the data\n",
    "        if sample_size is not None:\n",
    "            assert sample_size % 128 == 0\n",
    "            self.train, self.test = train.sample(sample_size), test.sample(sample_size)\n",
    "            # train, test = map(lambda df: df.sample(sample_size), [train, test])\n",
    "        \n",
    "        # prepare the data\n",
    "        ((self.train_x, self.train_y),\n",
    "         (self.test_x, self.test_y)) = map(self._prepare, [self.train, self.test])\n",
    "        \n",
    "        # pad the data as needed\n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        ((self.train_x, self.train_x_token_types),\n",
    "         (self.test_x, self.test_x_token_types)) = map(self._pad, \n",
    "                                                       [self.train_x, self.test_x])\n",
    "\n",
    "    def _prepare(self, df):\n",
    "        x, y = [], []\n",
    "        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
    "            for ndx, row in df.iterrows():\n",
    "                # get text and label from row of data\n",
    "                text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]\n",
    "                \n",
    "                # tokenize\n",
    "                tokens = self.tokenizer.tokenize(text)\n",
    "                \n",
    "                # append special start and end tokens\n",
    "                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "                \n",
    "                # convert tokens to IDs\n",
    "                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "                \n",
    "                # clip texts to keep it under the max sequence length threshold\n",
    "                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "                \n",
    "                # save results\n",
    "                x.append(token_ids)\n",
    "                y.append(int(label))\n",
    "                pbar.update()\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    def _pad(self, ids):\n",
    "        # The function pads the texts if they are less than the max sequence length threshold\n",
    "        x, t = [], []\n",
    "        token_type_ids = [0] * self.max_seq_len\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "            t.append(token_type_ids)\n",
    "        return np.array(x), np.array(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_Bp3oJBpP6k"
   },
   "source": [
    "## We'll come back to this later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8kAfi-ipP6k"
   },
   "source": [
    "**Goal:** _Leverage a **bidirectional language model** with a **transformer architecture** for <span style=\"color:green;font-weight: bold\">text classification</span> using **transfer learning**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uvhwt7xopP6l"
   },
   "source": [
    "# 2. Transfer Learning\n",
    "\n",
    "This was yesterday's topic so we won't go into too much detail.\n",
    "\n",
    "Transfer learning \"focuses on storing knowledge gained while solving one problem and applying it to a different but related problem\" (Wikipedia).\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <img src=\"img/math.jpeg\" style=\"width: 400px;\"/>\n",
    "    <img src=\"img/cs.jpeg\" style=\"width: 400px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZoMoIvFVpP6m"
   },
   "source": [
    "**Goal:** _Leverage a **bidirectional language model** with a **transformer architecture** for <span style=\"color:green;font-weight: bold\">text classification</span> using <span style=\"color:green;font-weight: bold\">transfer learning</span>_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRjPvC6NpP6n"
   },
   "source": [
    "# 3. Bidirectional Language Model\n",
    "\n",
    "Bidirectional language modelling will be our \"first task\" in the transfer learning framework. \n",
    "\n",
    "We will transfer what we learn from bidirectional language modelling to the IMDb sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eoMolP-npP6o"
   },
   "source": [
    "## But What Is Bidirectional Language Modelling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlBD682zpP6r"
   },
   "source": [
    "## But What Is ~~Bidirectional~~ Language Modelling?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J9wz5nyepP6s"
   },
   "source": [
    "<img src=\"img/LM.jpeg\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xflK4g1WpP6s"
   },
   "source": [
    "## But What Is Bidirectional Language Modelling?\n",
    "\n",
    "<img src=\"img/BiLM.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQO566ehpP6t"
   },
   "source": [
    "This task is self-supervised, which means it can be scaled to enormous amounts of data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTcyxVPUpP6u"
   },
   "source": [
    "**Goal:** _Leverage a <span style=\"color:green;font-weight: bold\">bidirectional language model</span> with a **transformer architecture** for <span style=\"color:green;font-weight:bold\">text classification</span> using <span style=\"color:green;font-weight: bold\">transfer learning</span>_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lC7VPV0WpP6x"
   },
   "source": [
    "# 4. Transformer Architecture\n",
    "\n",
    "Before you can understand transformers, you need to understand attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kkJU_BU_pP6y"
   },
   "source": [
    "## Dot Product Attention\n",
    "<img src=\"img/attention.png\"/>\n",
    "https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHggIB6QpP6z"
   },
   "source": [
    "## Dot Product Attention\n",
    "\n",
    "Can capture long-range dependencies better than recurrent models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mk2JjeGGpP60"
   },
   "source": [
    "## Transformers\n",
    "<img src=\"img/attention_all.png\" style=\"width:600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FLIPFoNpP61"
   },
   "source": [
    "## Transformers\n",
    "<img src=\"img/transformer.png\" style=\"width:300px\"/>\n",
    "Improving Language Understanding by Generative Pre-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSKHAY3upP61"
   },
   "source": [
    "## Self Attention\n",
    "It's almost attention, but everything is in terms of the input tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9w5Z3xmpP62"
   },
   "source": [
    "<img src=\"img/attention.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPvtNQerpP63"
   },
   "source": [
    "# BERT\n",
    "\n",
    "BERT basically just uses a transformer architecture for bidirectional language modelling.\n",
    "\n",
    "Trained on over 3 billion words!\n",
    "\n",
    "Google has released the model on tensorflow-hub.\n",
    "\n",
    "To transfer BERT to sentence level tasks like sentiment analysis, we represent the sentence with the encoding for the class token.\n",
    "\n",
    "<img src=\"img/BiLM.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tpVYfbacpP64"
   },
   "source": [
    "**Goal:** _Leverage a <span style=\"color:green;font-weight: bold\">bidirectional language model</span> with a <span style=\"color:green;font-weight: bold\">transformer architecture</span> for <span style=\"color:green;font-weight:bold\">text classification</span> using <span style=\"color:green;font-weight: bold\">transfer learning</span>_\n",
    "\n",
    "Hurray! Now we can code it up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVetGZPOpP65"
   },
   "source": [
    "# 5. Put it all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SOfyGHfwpP66"
   },
   "source": [
    "## Load BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qv8FIUlmpP66"
   },
   "outputs": [],
   "source": [
    "bert_ckpt_dir=\"gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/\"\n",
    "bert_ckpt_file = bert_ckpt_dir + \"bert_model.ckpt\"\n",
    "bert_config_file = bert_ckpt_dir + \"bert_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "cRjbFGbhpP69",
    "outputId": "96b6ea28-fc70-40fa-a931-860d204eba40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_config.json...\n",
      "/ [0 files][    0.0 B/  313.0 B]                                                \r",
      "/ [1 files][  313.0 B/  313.0 B]                                                \r\n",
      "Operation completed over 1 objects/313.0 B.                                      \n",
      "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/vocab.txt...\n",
      "/ [1 files][226.1 KiB/226.1 KiB]                                                \n",
      "Operation completed over 1 objects/226.1 KiB.                                    \n",
      "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt.meta...\n",
      "/ [1 files][883.1 KiB/883.1 KiB]                                                \n",
      "Operation completed over 1 objects/883.1 KiB.                                    \n",
      "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt.index...\n",
      "/ [1 files][  8.3 KiB/  8.3 KiB]                                                \n",
      "Operation completed over 1 objects/8.3 KiB.                                      \n",
      "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001...\n",
      "/ [1 files][420.0 MiB/420.0 MiB]                                                \n",
      "Operation completed over 1 objects/420.0 MiB.                                    \n",
      ".model:\n",
      "total 12\n",
      "drwxr-xr-x 3 root root 4096 Jan 14 17:41 .\n",
      "drwxr-xr-x 1 root root 4096 Jan 14 17:53 ..\n",
      "drwxr-xr-x 2 root root 4096 Jan 14 18:30 uncased_L-12_H-768_A-12\n",
      "\n",
      ".model/uncased_L-12_H-768_A-12:\n",
      "total 431244\n",
      "drwxr-xr-x 2 root root      4096 Jan 14 18:30 .\n",
      "drwxr-xr-x 3 root root      4096 Jan 14 17:41 ..\n",
      "-rw-r--r-- 1 root root       313 Jan 14 18:30 bert_config.json\n",
      "-rw-r--r-- 1 root root 440425712 Jan 14 18:30 bert_model.ckpt.data-00000-of-00001\n",
      "-rw-r--r-- 1 root root      8528 Jan 14 18:30 bert_model.ckpt.index\n",
      "-rw-r--r-- 1 root root    904243 Jan 14 18:30 bert_model.ckpt.meta\n",
      "-rw-r--r-- 1 root root    231508 Jan 14 18:30 vocab.txt\n",
      "CPU times: user 117 ms, sys: 53.3 ms, total: 170 ms\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bert_model_dir=\"2018_10_18\"\n",
    "bert_model_name=\"uncased_L-12_H-768_A-12\"\n",
    "\n",
    "!mkdir -p .model .model/$bert_model_name\n",
    "\n",
    "for fname in [\"bert_config.json\", \"vocab.txt\", \"bert_model.ckpt.meta\", \"bert_model.ckpt.index\", \"bert_model.ckpt.data-00000-of-00001\"]:\n",
    "  cmd = f\"gsutil cp gs://bert_models/{bert_model_dir}/{bert_model_name}/{fname} .model/{bert_model_name}\"\n",
    "  !$cmd\n",
    "\n",
    "!ls -la .model .model/$bert_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HxSf6OHOplHo"
   },
   "outputs": [],
   "source": [
    "bert_ckpt_dir    = os.path.join(\".model/\",bert_model_name)\n",
    "bert_ckpt_file   = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
    "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNf3IJg6pspu"
   },
   "source": [
    "# Prep the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "PsuOeaXvpsVR",
    "outputId": "a5896c5c-a3e5-4312-ff81-507153796ca8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos: 100%|██████████| 12500/12500 [00:01<00:00, 9014.76it/s]\n",
      "neg: 100%|██████████| 12500/12500 [00:01<00:00, 9039.40it/s]\n",
      "pos: 100%|██████████| 12500/12500 [00:01<00:00, 7927.50it/s]\n",
      "neg: 100%|██████████| 12500/12500 [00:01<00:00, 8201.28it/s]\n",
      "100%|██████████| 1.02k/1.02k [00:03<00:00, 292it/s]\n",
      "100%|██████████| 1.02k/1.02k [00:03<00:00, 283it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 2900\n",
      "CPU times: user 24.9 s, sys: 9.06 s, total: 34 s\n",
      "Wall time: 48.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "data = MovieReviewData(tokenizer, \n",
    "                       sample_size=1024,\n",
    "                       max_seq_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKZacAIRCZ-p"
   },
   "source": [
    "# Let's take a look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOzwPJytCc--"
   },
   "source": [
    "before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "id": "b11deojpqMwH",
    "outputId": "a54f0386-6589-46ef-fe71-8ada29c03520"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9660</th>\n",
       "      <td>I had high hopes for this film, since it has Charlton Heston and Jack Palance. But those hopes came crashing to earth in the first 20 minutes or so. Palance was ridiculous. Not even Heston's acting or Annabel Schofield's beauty (or brief nude scenes) could save this film. Some of the space effects were quite good, but others were cheesy. The plot was ludicrous. Even sci-fi fans should skip this one. Grade F</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23516</th>\n",
       "      <td>I caught this film at the Edinburgh Film Festival. I hadn't heard much about it; only that it was a tightly-paced thriller, shot digitally on a very low budget. I was hoping to catch the next big Brit-Flick. But I have to say, I was severely disappointed. \"This Is Not A Love Song\" follows two criminals, who, after accidentally shooting and killing a farmer's young daughter, become embroiled in a deadly game of cat and mouse when the locals decide to take matters into their own hands and hunt them down.&lt;br /&gt;&lt;br /&gt;The real problem is that this is yet another example of style over substance in a British film. The camera angles and editing are completely at odds with the story, as are the over the top performances, and the appalling use of slow motion, which only serves to make the whole thing look like an expensive home video. There are repeated attempts to make the film look edgy and gritty, which instead come over as hilarious and over the top(Cue a pathetic, obligatory drug scene, and countless, pointless camera zooms). No amount of cliche's such as this can disguise the fact that this is a pretty bad story.&lt;br /&gt;&lt;br /&gt;We've seen this kind of thing many times before, and made a hundred times better, particularly in John Boorman's masterful \"Deliverance.\" But while in the latter film, we actually cared about the characters, in this film, I found myself just wanting them to be hunted down and killed as quickly as possible. Even this wouldn't have been so bad if their adversaries had been frightening or worthwhile, but instead, are merely a collection of stereotypical, inbred-looking countryfolk. Again, another offensive, overused cliche' coming to the fore. Surely there are some nice people in the country, filmmakers?&lt;br /&gt;&lt;br /&gt;In its defense, \"This Is Not A Love Song\" does contain a couple of good, suspenseful moments, but it's hard to see this film doing anything other than going straight to video, or, at a push, getting a very limited cinema release. It's not a patch on last year's Low-Budget hunted in the hills movie, \"Dog soldiers\". Maybe British Cinema could actually get kick-started again if the right money stopped going to the wrong people.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         sentence  ... polarity\n",
       "9660   I had high hopes for this film, since it has Charlton Heston and Jack Palance. But those hopes came crashing to earth in the first 20 minutes or so. Palance was ridiculous. Not even Heston's acting or Annabel Schofield's beauty (or brief nude scenes) could save this film. Some of the space effects were quite good, but others were cheesy. The plot was ludicrous. Even sci-fi fans should skip this one. Grade F                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ...  0      \n",
       "23516  I caught this film at the Edinburgh Film Festival. I hadn't heard much about it; only that it was a tightly-paced thriller, shot digitally on a very low budget. I was hoping to catch the next big Brit-Flick. But I have to say, I was severely disappointed. \"This Is Not A Love Song\" follows two criminals, who, after accidentally shooting and killing a farmer's young daughter, become embroiled in a deadly game of cat and mouse when the locals decide to take matters into their own hands and hunt them down.<br /><br />The real problem is that this is yet another example of style over substance in a British film. The camera angles and editing are completely at odds with the story, as are the over the top performances, and the appalling use of slow motion, which only serves to make the whole thing look like an expensive home video. There are repeated attempts to make the film look edgy and gritty, which instead come over as hilarious and over the top(Cue a pathetic, obligatory drug scene, and countless, pointless camera zooms). No amount of cliche's such as this can disguise the fact that this is a pretty bad story.<br /><br />We've seen this kind of thing many times before, and made a hundred times better, particularly in John Boorman's masterful \"Deliverance.\" But while in the latter film, we actually cared about the characters, in this film, I found myself just wanting them to be hunted down and killed as quickly as possible. Even this wouldn't have been so bad if their adversaries had been frightening or worthwhile, but instead, are merely a collection of stereotypical, inbred-looking countryfolk. Again, another offensive, overused cliche' coming to the fore. Surely there are some nice people in the country, filmmakers?<br /><br />In its defense, \"This Is Not A Love Song\" does contain a couple of good, suspenseful moments, but it's hard to see this film doing anything other than going straight to video, or, at a push, getting a very limited cinema release. It's not a patch on last year's Low-Budget hunted in the hills movie, \"Dog soldiers\". Maybe British Cinema could actually get kick-started again if the right money stopped going to the wrong people.  ...  0      \n",
       "\n",
       "[2 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6tJewiRCfR7"
   },
   "source": [
    "after..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "sRs2voiCqGd1",
    "outputId": "ba33e7bc-88d2-4289-d272-c1deea69caf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x:\n",
      " [[  101  1045  2018 ...     0     0     0]\n",
      " [  101  1045  3236 ...  3291     0     0]\n",
      " [  101  1045  2245 ...  3531     0     0]\n",
      " ...\n",
      " [  101  1037 10103 ...  1028     0     0]\n",
      " [  101  2023  2003 ...  1045     0     0]\n",
      " [  101  1996  2197 ...  2250     0     0]]\n",
      "train_x shape: (1024, 128)\n",
      "train_x_token_types:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "train_x_token_types shape: (1024, 128)\n",
      "train_y: [0 0 1 ... 0 1 1]\n",
      "train_y shape: (1024,)\n",
      "max_seq_len: 128\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x:\\n\", data.train_x)\n",
    "print(\"train_x shape:\", data.train_x.shape)\n",
    "print(\"train_x_token_types:\\n\", data.train_x_token_types)\n",
    "print(\"train_x_token_types shape:\", data.train_x_token_types.shape)\n",
    "print(\"train_y:\", data.train_y)\n",
    "print(\"train_y shape:\", data.train_y.shape)\n",
    "print(\"max_seq_len:\", data.max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rLeEYAHERKl"
   },
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KunuVvLDOkI3"
   },
   "source": [
    "Initialize BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_Qf9s8nOXoJ"
   },
   "outputs": [],
   "source": [
    "with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "      bc = StockBertConfig.from_json_string(reader.read())\n",
    "      bert_params = map_stock_config_to_params(bc)\n",
    "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-v5m-8bOl9y"
   },
   "source": [
    "Build model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LE_CbmsJOaz-"
   },
   "outputs": [],
   "source": [
    "input_ids = keras.layers.Input(shape=(data.max_seq_len,), dtype='int32', name=\"input_ids\")\n",
    "output = bert(input_ids)\n",
    "\n",
    "cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
    "cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "logits = keras.layers.Dense(units=768, activation=\"relu\")(cls_out)\n",
    "logits = keras.layers.Dropout(0.5)(logits)\n",
    "logits = keras.layers.Dense(units=2, activation=\"softmax\")(logits)\n",
    "\n",
    "model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "model.build(input_shape=(None, data.max_seq_len))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K7VIfIGCPM65"
   },
   "source": [
    "Load BERT weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "cl7P59ouPMdr",
    "outputId": "10d446e2-fa96-4850-90ca-652cb7103afd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading 196 BERT weights from: .model/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7fa183607470> (prefix:bert_1). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_stock_weights(bert, bert_ckpt_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yMMCe0TxPUZX"
   },
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9--ZmrAPWaa"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOATrCvFP3fJ"
   },
   "source": [
    "Let's take a look at the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "IJHcWHsIP5DL",
    "outputId": "21c1ee8e-75b3-44e3-e2a4-58e689f5b5fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1538      \n",
      "=================================================================\n",
      "Total params: 109,482,242\n",
      "Trainable params: 109,482,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0E9hnK4QQmjt"
   },
   "source": [
    "Define learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_xnMQBUDeCi"
   },
   "outputs": [],
   "source": [
    "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                   end_learn_rate=1e-7,\n",
    "                                   warmup_epoch_count=10,\n",
    "                                   total_epoch_count=90):\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < warmup_epoch_count:\n",
    "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
    "        else:\n",
    "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
    "        return float(res)\n",
    "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "    return learning_rate_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k32Ta7UEE8kf"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "elDDOYBYE9q2",
    "outputId": "22fc8893-21bf-4009-ebd0-7e6ccbdfcf94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 921 samples, validate on 103 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 5.000000000000001e-07.\n",
      "Epoch 1/50\n",
      "921/921 [==============================] - 22s 24ms/sample - loss: 0.7150 - acc: 0.4832 - val_loss: 0.7133 - val_acc: 0.4563\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 1.0000000000000002e-06.\n",
      "Epoch 2/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.7257 - acc: 0.4886 - val_loss: 0.7028 - val_acc: 0.4660\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 1.5000000000000002e-06.\n",
      "Epoch 3/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.7144 - acc: 0.5005 - val_loss: 0.6924 - val_acc: 0.5049\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 2.0000000000000003e-06.\n",
      "Epoch 4/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.7035 - acc: 0.5353 - val_loss: 0.6832 - val_acc: 0.5437\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 2.5000000000000006e-06.\n",
      "Epoch 5/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.6914 - acc: 0.5385 - val_loss: 0.6741 - val_acc: 0.6408\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 3.0000000000000005e-06.\n",
      "Epoch 6/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.6858 - acc: 0.5527 - val_loss: 0.6647 - val_acc: 0.6505\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 3.5000000000000004e-06.\n",
      "Epoch 7/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.6782 - acc: 0.5700 - val_loss: 0.6546 - val_acc: 0.7087\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 4.000000000000001e-06.\n",
      "Epoch 8/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.6669 - acc: 0.5972 - val_loss: 0.6385 - val_acc: 0.7379\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 4.500000000000001e-06.\n",
      "Epoch 9/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.6562 - acc: 0.6113 - val_loss: 0.6123 - val_acc: 0.7379\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 5.000000000000001e-06.\n",
      "Epoch 10/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.6222 - acc: 0.6612 - val_loss: 0.5653 - val_acc: 0.7767\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 5.500000000000001e-06.\n",
      "Epoch 11/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.5728 - acc: 0.7372 - val_loss: 0.5496 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 6.000000000000001e-06.\n",
      "Epoch 12/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.5253 - acc: 0.7752 - val_loss: 0.4895 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 6.500000000000001e-06.\n",
      "Epoch 13/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.4792 - acc: 0.8436 - val_loss: 0.4747 - val_acc: 0.8447\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 7.000000000000001e-06.\n",
      "Epoch 14/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.4534 - acc: 0.8708 - val_loss: 0.4978 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 7.500000000000001e-06.\n",
      "Epoch 15/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.4297 - acc: 0.8817 - val_loss: 0.4743 - val_acc: 0.8447\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 8.000000000000001e-06.\n",
      "Epoch 16/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.4075 - acc: 0.9099 - val_loss: 0.5228 - val_acc: 0.7864\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 8.500000000000002e-06.\n",
      "Epoch 17/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3904 - acc: 0.9283 - val_loss: 0.4754 - val_acc: 0.8350\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 9.000000000000002e-06.\n",
      "Epoch 18/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3864 - acc: 0.9316 - val_loss: 0.4705 - val_acc: 0.8350\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 9.500000000000002e-06.\n",
      "Epoch 19/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3816 - acc: 0.9316 - val_loss: 0.4679 - val_acc: 0.8447\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 1.0000000000000003e-05.\n",
      "Epoch 20/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3810 - acc: 0.9305 - val_loss: 0.4508 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 8.619535664753031e-06.\n",
      "Epoch 21/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3750 - acc: 0.9403 - val_loss: 0.5432 - val_acc: 0.7573\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 7.429639507594948e-06.\n",
      "Epoch 22/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3737 - acc: 0.9403 - val_loss: 0.5020 - val_acc: 0.8058\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 6.404004271197281e-06.\n",
      "Epoch 23/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3531 - acc: 0.9642 - val_loss: 0.4571 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 5.51995432128157e-06.\n",
      "Epoch 24/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3528 - acc: 0.9631 - val_loss: 0.4528 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 4.757944314009411e-06.\n",
      "Epoch 25/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3518 - acc: 0.9653 - val_loss: 0.5032 - val_acc: 0.7961\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 4.101127070551301e-06.\n",
      "Epoch 26/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3475 - acc: 0.9653 - val_loss: 0.4524 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 3.5349811050301065e-06.\n",
      "Epoch 27/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3414 - acc: 0.9729 - val_loss: 0.4510 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 3.046989570903508e-06.\n",
      "Epoch 28/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3434 - acc: 0.9685 - val_loss: 0.4490 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 2.626363527653332e-06.\n",
      "Epoch 29/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3403 - acc: 0.9729 - val_loss: 0.4543 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 2.263803409521448e-06.\n",
      "Epoch 30/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3403 - acc: 0.9739 - val_loss: 0.4488 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1.9512934226359633e-06.\n",
      "Epoch 31/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3357 - acc: 0.9794 - val_loss: 0.4486 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1.6819243248808695e-06.\n",
      "Epoch 32/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3350 - acc: 0.9805 - val_loss: 0.4486 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1.4497406703726316e-06.\n",
      "Epoch 33/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3368 - acc: 0.9772 - val_loss: 0.4466 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 1.2496091412919872e-06.\n",
      "Epoch 34/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3374 - acc: 0.9772 - val_loss: 0.4476 - val_acc: 0.8447\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 1.0771050560367686e-06.\n",
      "Epoch 35/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3363 - acc: 0.9783 - val_loss: 0.4466 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 9.284145445194742e-07.\n",
      "Epoch 36/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3365 - acc: 0.9772 - val_loss: 0.4465 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 8.002502278161051e-07.\n",
      "Epoch 37/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3358 - acc: 0.9794 - val_loss: 0.4468 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 6.897785379387655e-07.\n",
      "Epoch 38/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3352 - acc: 0.9794 - val_loss: 0.4473 - val_acc: 0.8544\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 5.945570708544392e-07.\n",
      "Epoch 39/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3350 - acc: 0.9794 - val_loss: 0.4469 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 5.124805876960932e-07.\n",
      "Epoch 40/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3363 - acc: 0.9772 - val_loss: 0.4469 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 4.417344703140068e-07.\n",
      "Epoch 41/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3350 - acc: 0.9794 - val_loss: 0.4470 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 3.807546021222372e-07.\n",
      "Epoch 42/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3348 - acc: 0.9794 - val_loss: 0.4472 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 3.281927872511473e-07.\n",
      "Epoch 43/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3364 - acc: 0.9783 - val_loss: 0.4474 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 2.8288694346259687e-07.\n",
      "Epoch 44/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3355 - acc: 0.9783 - val_loss: 0.4477 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 2.4383540982688284e-07.\n",
      "Epoch 45/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3364 - acc: 0.9794 - val_loss: 0.4465 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 2.1017480113324875e-07.\n",
      "Epoch 46/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3354 - acc: 0.9783 - val_loss: 0.4466 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.8116091942004138e-07.\n",
      "Epoch 47/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3349 - acc: 0.9794 - val_loss: 0.4466 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.5615230060004974e-07.\n",
      "Epoch 48/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3355 - acc: 0.9794 - val_loss: 0.4474 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1.3459603241553634e-07.\n",
      "Epoch 49/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3363 - acc: 0.9783 - val_loss: 0.4467 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.1601553017399705e-07.\n",
      "Epoch 50/50\n",
      "921/921 [==============================] - 14s 16ms/sample - loss: 0.3358 - acc: 0.9794 - val_loss: 0.4463 - val_acc: 0.8738\n",
      "CPU times: user 6min 5s, sys: 3min 10s, total: 9min 16s\n",
      "Wall time: 12min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "log_dir = \".log/movie_reviews/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "total_epoch_count = 50\n",
    "# model.fit(x=(data.train_x, data.train_x_token_types), y=data.train_y,\n",
    "model.fit(x=data.train_x, y=data.train_y,\n",
    "          validation_split=0.1,\n",
    "          batch_size=48,\n",
    "          shuffle=True,\n",
    "          epochs=total_epoch_count,\n",
    "          callbacks=[create_learning_rate_scheduler(max_learn_rate=1e-5,\n",
    "                                                    end_learn_rate=1e-7,\n",
    "                                                    warmup_epoch_count=20,\n",
    "                                                    total_epoch_count=total_epoch_count),\n",
    "                     keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "                     tensorboard_callback])\n",
    "\n",
    "model.save_weights('./movie_reviews.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z44gWnhxMAgQ"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "FtzgJPQlL5ix",
    "outputId": "367d5f23-dacd-4119-89dc-cb2143943955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024/1024 [==============================] - 6s 6ms/sample - loss: 0.3452 - acc: 0.9688\n",
      "1024/1024 [==============================] - 6s 6ms/sample - loss: 0.4882 - acc: 0.8213\n",
      "train acc 0.96875\n",
      " test acc 0.82128906\n",
      "CPU times: user 3.47 s, sys: 697 ms, total: 4.16 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# model = create_model(data.max_seq_len, adapter_size=None)\n",
    "# model.load_weights(\"movie_reviews.h5\")\n",
    "\n",
    "_, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "print(\"train acc\", train_acc)\n",
    "print(\" test acc\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YY9OHTpZMEKK"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "LmDNf2vPM7Nu",
    "outputId": "e3ab6d28-843a-46af-95c7-1dae5f6e1410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_token_ids (4, 128)\n",
      " text: That movie was absolutely awful\n",
      "  res: negative\n",
      " text: The movie was so boring\n",
      "  res: negative\n",
      " text: The film was creative and surprising\n",
      "  res: positive\n",
      " text: Absolutely fantastic!\n",
      "  res: positive\n"
     ]
    }
   ],
   "source": [
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The movie was so boring\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\"\n",
    "]\n",
    "\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "pred_tokens    = map(tokenizer.tokenize, pred_sentences)\n",
    "pred_tokens    = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
    "pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
    "\n",
    "pred_token_ids = map(lambda tids: tids +[0]*(data.max_seq_len-len(tids)),pred_token_ids)\n",
    "pred_token_ids = np.array(list(pred_token_ids))\n",
    "\n",
    "print('pred_token_ids', pred_token_ids.shape)\n",
    "\n",
    "res = model.predict(pred_token_ids).argmax(axis=-1)\n",
    "\n",
    "for text, sentiment in zip(pred_sentences, res):\n",
    "  print(\" text:\", text)\n",
    "  print(\"  res:\", [\"negative\",\"positive\"][sentiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MP3HwMsM7ei"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [
    "eoMolP-npP6o",
    "kkJU_BU_pP6y",
    "SHggIB6QpP6z",
    "mk2JjeGGpP60",
    "2FLIPFoNpP61",
    "TLMpFIMhpP7b",
    "AujYRzb0pP7q",
    "MIWgLpsKpP7v",
    "4yaoSy1kpP73"
   ],
   "name": "Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
