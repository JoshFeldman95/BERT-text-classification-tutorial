{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "to run presentation, enter `jupyter nbconvert Tutorial.ipynb --to slides --post serve` in the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Classification with BERT\n",
    "\n",
    "<img src=\"img/bert_reading.jpg\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop Goal in Machine Learning Jargon:\n",
    "\n",
    "_Leverage a **bidirectional language model** with a **transformer architecture** for **text classification** using **transfer learning**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Right now, this should make no sense!\n",
    "\n",
    "But by the end of this workshop, you will not only understand what this means, but you will be able to use this method to train a natural language processing model with only 500 data points.\n",
    "\n",
    "**Note:** This code in this tutorial is largely based on Google's tutorial available [here](https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb) and this Towards Data Science post available [here](https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Road Ahead\n",
    "\n",
    "**Goal:** _Leverage a **bidirectional language model** with a **transformer architecture** for **text classification** using **transfer learning**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Text Classification\n",
    "2. Transfer Learning\n",
    "3. Bidirectional Language Model\n",
    "4. Transformer Architecture\n",
    "5. Put it all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Text classification is... classifying text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentiment analysis:\n",
    "<img src=\"img/harvard_review.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic classification\n",
    "<img src=\"img/topic_classification.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IMDb Sentiment Analysis\n",
    "\n",
    "Today, we're going to be working with the IMDb sentiment analysis dataset.\n",
    "\n",
    "**The Task:** Classify a movie review on IMDb as <span style=\"color:green\">positive</span> or <span style=\"color:red\">negative</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading the Data\n",
    "\n",
    "We will be using the IMDB Large Movie Review Dataset, which is hosted by Stanford."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Look into hosting data ourselves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in tqdm(os.listdir(directory), desc=os.path.basename(directory)):\n",
    "        with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now define a class that will handle downloading and preparing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "class MovieReviewData:\n",
    "    DATA_COLUMN = \"sentence\"\n",
    "    LABEL_COLUMN = \"polarity\"\n",
    "\n",
    "    def __init__(self, tokenizer: FullTokenizer, sample_size=None, max_seq_len=128):\n",
    "        # Download the data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_size = sample_size\n",
    "        self.max_seq_len = 0\n",
    "        train, test = download_and_load_datasets()\n",
    "        \n",
    "        # Re-index by length of text\n",
    "        train, test = map(lambda df: \n",
    "                          df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index), \n",
    "                          [train, test])\n",
    "        \n",
    "        # sample the data\n",
    "        if sample_size is not None:\n",
    "            assert sample_size % 128 == 0\n",
    "            train, test = train.head(sample_size), test.head(sample_size)\n",
    "            # train, test = map(lambda df: df.sample(sample_size), [train, test])\n",
    "        \n",
    "        # prepare the data\n",
    "        ((self.train_x, self.train_y),\n",
    "         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "        \n",
    "        # pad the data as needed\n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        ((self.train_x, self.train_x_token_types),\n",
    "         (self.test_x, self.test_x_token_types)) = map(self._pad, \n",
    "                                                       [self.train_x, self.test_x])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "def _prepare(self, df):\n",
    "    x, y = [], []\n",
    "    with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
    "        for ndx, row in df.iterrows():\n",
    "            # get text and label from row of data\n",
    "            text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]\n",
    "\n",
    "            # tokenize\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "            # append special start and end tokens\n",
    "            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "\n",
    "            # convert tokens to IDs\n",
    "            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            # clip texts to keep it under the max sequence length threshold\n",
    "            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "\n",
    "            # save results\n",
    "            x.append(token_ids)\n",
    "            y.append(int(label))\n",
    "            pbar.update()\n",
    "    return np.array(x), np.array(y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "def _pad(self, ids):\n",
    "        # The function pads the texts if they are less than the max sequence length threshold\n",
    "        x, t = [], []\n",
    "        token_type_ids = [0] * self.max_seq_len\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "            t.append(token_type_ids)\n",
    "        return np.array(x), np.array(t)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class MovieReviewData:\n",
    "    DATA_COLUMN = \"sentence\"\n",
    "    LABEL_COLUMN = \"polarity\"\n",
    "\n",
    "    def __init__(self, tokenizer, sample_size=None, max_seq_len=128):\n",
    "        # Download the data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_size = sample_size\n",
    "        self.max_seq_len = 0\n",
    "        train, test = download_and_load_datasets()\n",
    "        \n",
    "        # Re-index by length of text\n",
    "        train, test = map(lambda df: df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index), \n",
    "                          [train, test])\n",
    "        \n",
    "        # sample the data\n",
    "        if sample_size is not None:\n",
    "            assert sample_size % 128 == 0\n",
    "            train, test = train.sample(sample_size), test.sample(sample_size)\n",
    "            # train, test = map(lambda df: df.sample(sample_size), [train, test])\n",
    "        \n",
    "        # prepare the data\n",
    "        ((self.train_x, self.train_y),\n",
    "         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "        \n",
    "        # pad the data as needed\n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        ((self.train_x, self.train_x_token_types),\n",
    "         (self.test_x, self.test_x_token_types)) = map(self._pad, \n",
    "                                                       [self.train_x, self.test_x])\n",
    "\n",
    "    def _prepare(self, df):\n",
    "        x, y = [], []\n",
    "        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
    "            for ndx, row in df.iterrows():\n",
    "                # get text and label from row of data\n",
    "                text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]\n",
    "                \n",
    "                # tokenize\n",
    "                tokens = self.tokenizer.tokenize(text)\n",
    "                \n",
    "                # append special start and end tokens\n",
    "                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "                \n",
    "                # convert tokens to IDs\n",
    "                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "                \n",
    "                # clip texts to keep it under the max sequence length threshold\n",
    "                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "                \n",
    "                # save results\n",
    "                x.append(token_ids)\n",
    "                y.append(int(label))\n",
    "                pbar.update()\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    def _pad(self, ids):\n",
    "        # The function pads the texts if they are less than the max sequence length threshold\n",
    "        x, t = [], []\n",
    "        token_type_ids = [0] * self.max_seq_len\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "            t.append(token_type_ids)\n",
    "        return np.array(x), np.array(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We'll come back to this later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Goal:** _Leverage a **bidirectional language model** with a **transformer architecture** for <span style=\"color:green;font-weight: bold\">text classification</span> using **transfer learning**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Transfer Learning\n",
    "\n",
    "This was yesterday's topic so we won't go into too much detail.\n",
    "\n",
    "Transfer learning \"focuses on storing knowledge gained while solving one problem and applying it to a different but related problem\" (Wikipedia).\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <img src=\"img/math.jpeg\" style=\"width: 400px;\"/>\n",
    "    <img src=\"img/cs.jpeg\" style=\"width: 400px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Goal:** _Leverage a **bidirectional language model** with a **transformer architecture** for <span style=\"color:green;font-weight: bold\">text classification</span> using <span style=\"color:green;font-weight: bold\">transfer learning</span>_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Bidirectional Language Model\n",
    "\n",
    "Bidirectional language modelling will be our \"first task\" in the transfer learning framework. \n",
    "\n",
    "We will transfer what we learn from bidirectional language modelling to the IMDb sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But What Is Bidirectional Language Modelling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But What Is ~~Bidirectional~~ Language Modelling?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/LM.jpeg\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But What Is Bidirectional Language Modelling?\n",
    "\n",
    "<img src=\"img/BiLM.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This task is self-supervised, which means it can be scaled to enormous amounts of data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Goal:** _Leverage a <span style=\"color:green;font-weight: bold\">bidirectional language model</span> with a **transformer architecture** for <span style=\"color:green;font-weight:bold\">text classification</span> using <span style=\"color:green;font-weight: bold\">transfer learning</span>_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Transformer Architecture\n",
    "\n",
    "Before you can understand transformers, you need to understand attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dot Product Attention\n",
    "<img src=\"img/attention.png\"/>\n",
    "https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product Attention\n",
    "\n",
    "Can capture long-range dependencies better than recurrent models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "<img src=\"img/attention_all.png\" style=\"width:600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "<img src=\"img/transformer.png\" style=\"width:300px\"/>\n",
    "Improving Language Understanding by Generative Pre-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self Attention\n",
    "It's almost attention, but everything is in terms of the input tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/attention.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BERT\n",
    "\n",
    "BERT basically just uses a transformer architecture for bidirectional language modelling.\n",
    "\n",
    "Trained on over 3 billion words!\n",
    "\n",
    "Google has released the model on tensorflow-hub.\n",
    "\n",
    "To transfer BERT to sentence level tasks like sentiment analysis, we represent the sentence with the encoding for the class token.\n",
    "\n",
    "<img src=\"img/BiLM.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Goal:** _Leverage a <span style=\"color:green;font-weight: bold\">bidirectional language model</span> with a <span style=\"color:green;font-weight: bold\">transformer architecture</span> for <span style=\"color:green;font-weight:bold\">text classification</span> using <span style=\"color:green;font-weight: bold\">transfer learning</span>_\n",
    "\n",
    "Hurray! Now we can code it up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Put it all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ckpt_dir=\"gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/\"\n",
    "bert_ckpt_file = bert_ckpt_dir + \"bert_model.ckpt\"\n",
    "bert_config_file = bert_ckpt_dir + \"bert_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: gsutil: command not found\n",
      "/bin/sh: gsutil: command not found\n",
      "/bin/sh: gsutil: command not found\n",
      "/bin/sh: gsutil: command not found\n",
      "/bin/sh: gsutil: command not found\n",
      ".model:\n",
      "total 0\n",
      "drwxr-xr-x   3 joshfeldman  staff   96 14 Jan 10:07 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  14 joshfeldman  staff  448 14 Jan 10:12 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "drwxr-xr-x   2 joshfeldman  staff   64 14 Jan 10:07 \u001b[1m\u001b[36muncased_L-12_H-768_A-12\u001b[m\u001b[m\n",
      "\n",
      ".model/uncased_L-12_H-768_A-12:\n",
      "total 0\n",
      "drwxr-xr-x  2 joshfeldman  staff  64 14 Jan 10:07 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  3 joshfeldman  staff  96 14 Jan 10:07 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "CPU times: user 20.5 ms, sys: 46 ms, total: 66.4 ms\n",
      "Wall time: 820 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bert_model_dir=\"2018_10_18\"\n",
    "bert_model_name=\"uncased_L-12_H-768_A-12\"\n",
    "\n",
    "!mkdir -p .model .model/$bert_model_name\n",
    "\n",
    "for fname in [\"bert_config.json\", \"vocab.txt\", \"bert_model.ckpt.meta\", \"bert_model.ckpt.index\", \"bert_model.ckpt.data-00000-of-00001\"]:\n",
    "  cmd = f\"gsutil cp gs://bert_models/{bert_model_dir}/{bert_model_name}/{fname} .model/{bert_model_name}\"\n",
    "  !$cmd\n",
    "\n",
    "!ls -la .model .model/$bert_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128  # Your choice here.\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"mask_ids\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "                            output_shape=[20],  # Outputs a tensor with shape [batch_size, 20].\n",
    "                            input_shape=[]\n",
    "                            trainable=True,\n",
    "                            signature=\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.keras_layer.KerasLayer at 0x14fc8feb8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    /Users/joshfeldman/Harvard/ComputeFest/BERT_tutorial_env/lib/python3.6/site-packages/tensorflow_hub/keras_layer.py:209 call  *\n        result = f()\n    /Users/joshfeldman/Harvard/ComputeFest/BERT_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:1551 __call__  *\n        return self._call_impl(args, kwargs)\n    /Users/joshfeldman/Harvard/ComputeFest/BERT_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:1570 _call_impl\n        ).format(self._num_positional_args, self._arg_keywords, args))\n\n    TypeError: Expected at most 0 positional arguments (and the rest keywords, of ['segment_ids', 'input_mask', 'input_ids']), got ([<tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'mask_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'segment_ids_5:0' shape=(None, 128) dtype=int32>],). When calling a concrete function, positional arguments may not be bound to Tensors within nested structures.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d6ba3f7a8a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                             signature=\"tokens\")\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_word_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Harvard/ComputeFest/BERT_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    772\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Harvard/ComputeFest/BERT_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    /Users/joshfeldman/Harvard/ComputeFest/BERT_tutorial_env/lib/python3.6/site-packages/tensorflow_hub/keras_layer.py:209 call  *\n        result = f()\n    /Users/joshfeldman/Harvard/ComputeFest/BERT_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:1551 __call__  *\n        return self._call_impl(args, kwargs)\n    /Users/joshfeldman/Harvard/ComputeFest/BERT_tutorial_env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:1570 _call_impl\n        ).format(self._num_positional_args, self._arg_keywords, args))\n\n    TypeError: Expected at most 0 positional arguments (and the rest keywords, of ['segment_ids', 'input_mask', 'input_ids']), got ([<tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'mask_ids:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'segment_ids_5:0' shape=(None, 128) dtype=int32>],). When calling a concrete function, positional arguments may not be bound to Tensors within nested structures.\n"
     ]
    }
   ],
   "source": [
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.keras_layer.KerasLayer at 0x150dfbfd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download BERT from Tensorflow Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "bert_module = hub.Module(BERT_MODEL_HUB, trainable=True)\n",
    "tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "bert_layer = hub.KerasLayer(bert_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'do_lower_case': <tf.Tensor 'module_4_apply_tokenization_info/Const:0' shape=() dtype=bool>,\n",
       " 'vocab_file': <tf.Tensor 'module_4_apply_tokenization_info/vocab_file:0' shape=() dtype=string>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenization_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Load data as an InputExample object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the model and the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "\n",
    "    return bert.tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'bert',\n",
       " 'token',\n",
       " '##izer']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joshfeldman/Harvard/ComputeFest/BERT_tutorial_env/lib/python3.6/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/joshfeldman/Harvard/ComputeFest/BERT_tutorial_env/lib/python3.6/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this movie had so much potential - a strong cast , a reasonably strong idea and clearly a decent budget . i ' m not sure where it all went wrong , but each of those elements was wasted . the story went nowhere , the characters were hollow to say the least and the result was a very boring , pointless , waste of a film . i hated it . judging by the other votes , i ' m in the minority here and must be some sort of freak . however , i thought this movie was dreadful . i had high hopes , but was very disappointed . a particular disappointment was jody foster ' s character . a very cocky \" [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this movie had so much potential - a strong cast , a reasonably strong idea and clearly a decent budget . i ' m not sure where it all went wrong , but each of those elements was wasted . the story went nowhere , the characters were hollow to say the least and the result was a very boring , pointless , waste of a film . i hated it . judging by the other votes , i ' m in the minority here and must be some sort of freak . however , i thought this movie was dreadful . i had high hopes , but was very disappointed . a particular disappointment was jody foster ' s character . a very cocky \" [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 3185 2018 2061 2172 4022 1011 1037 2844 3459 1010 1037 16286 2844 2801 1998 4415 1037 11519 5166 1012 1045 1005 1049 2025 2469 2073 2009 2035 2253 3308 1010 2021 2169 1997 2216 3787 2001 13842 1012 1996 2466 2253 7880 1010 1996 3494 2020 8892 2000 2360 1996 2560 1998 1996 2765 2001 1037 2200 11771 1010 23100 1010 5949 1997 1037 2143 1012 1045 6283 2009 1012 13325 2011 1996 2060 4494 1010 1045 1005 1049 1999 1996 7162 2182 1998 2442 2022 2070 4066 1997 11576 1012 2174 1010 1045 2245 2023 3185 2001 21794 1012 1045 2018 2152 8069 1010 2021 2001 2200 9364 1012 1037 3327 10520 2001 27562 6469 1005 1055 2839 1012 1037 2200 24995 1000 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 3185 2018 2061 2172 4022 1011 1037 2844 3459 1010 1037 16286 2844 2801 1998 4415 1037 11519 5166 1012 1045 1005 1049 2025 2469 2073 2009 2035 2253 3308 1010 2021 2169 1997 2216 3787 2001 13842 1012 1996 2466 2253 7880 1010 1996 3494 2020 8892 2000 2360 1996 2560 1998 1996 2765 2001 1037 2200 11771 1010 23100 1010 5949 1997 1037 2143 1012 1045 6283 2009 1012 13325 2011 1996 2060 4494 1010 1045 1005 1049 1999 1996 7162 2182 1998 2442 2022 2070 4066 1997 11576 1012 2174 1010 1045 2245 2023 3185 2001 21794 1012 1045 2018 2152 8069 1010 2021 2001 2200 9364 1012 1037 3327 10520 2001 27562 6469 1005 1055 2839 1012 1037 2200 24995 1000 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] when you have a di ##se ##mbo ##die ##d skull , an empty mansion , a sc ##hi ##zo ##ph ##ren ##ic wife , a sc ##hem ##ing cad and a nut ##zo gardener , throw in a minister and his wife - what have you got ? < br / > < br / > ai ##p ' s answer to ins ##om ##nia . < br / > < br / > \" the screaming skull \" gets points for au ##da ##city , offering free casket ##s for anyone who dies of fright from watching the film . pretty safe bet , when you ' re lu ##lled into a stu ##por by people who think they ' re in a production of [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] when you have a di ##se ##mbo ##die ##d skull , an empty mansion , a sc ##hi ##zo ##ph ##ren ##ic wife , a sc ##hem ##ing cad and a nut ##zo gardener , throw in a minister and his wife - what have you got ? < br / > < br / > ai ##p ' s answer to ins ##om ##nia . < br / > < br / > \" the screaming skull \" gets points for au ##da ##city , offering free casket ##s for anyone who dies of fright from watching the film . pretty safe bet , when you ' re lu ##lled into a stu ##por by people who think they ' re in a production of [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2043 2017 2031 1037 4487 3366 13344 10265 2094 7412 1010 2019 4064 7330 1010 1037 8040 4048 6844 8458 7389 2594 2564 1010 1037 8040 29122 2075 28353 1998 1037 17490 6844 19785 1010 5466 1999 1037 2704 1998 2010 2564 1011 2054 2031 2017 2288 1029 1026 7987 1013 1028 1026 7987 1013 1028 9932 2361 1005 1055 3437 2000 16021 5358 6200 1012 1026 7987 1013 1028 1026 7987 1013 1028 1000 1996 7491 7412 1000 4152 2685 2005 8740 2850 12972 1010 5378 2489 25864 2015 2005 3087 2040 8289 1997 25966 2013 3666 1996 2143 1012 3492 3647 6655 1010 2043 2017 1005 2128 11320 11001 2046 1037 24646 17822 2011 2111 2040 2228 2027 1005 2128 1999 1037 2537 1997 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2043 2017 2031 1037 4487 3366 13344 10265 2094 7412 1010 2019 4064 7330 1010 1037 8040 4048 6844 8458 7389 2594 2564 1010 1037 8040 29122 2075 28353 1998 1037 17490 6844 19785 1010 5466 1999 1037 2704 1998 2010 2564 1011 2054 2031 2017 2288 1029 1026 7987 1013 1028 1026 7987 1013 1028 9932 2361 1005 1055 3437 2000 16021 5358 6200 1012 1026 7987 1013 1028 1026 7987 1013 1028 1000 1996 7491 7412 1000 4152 2685 2005 8740 2850 12972 1010 5378 2489 25864 2015 2005 3087 2040 8289 1997 25966 2013 3666 1996 2143 1012 3492 3647 6655 1010 2043 2017 1005 2128 11320 11001 2046 1037 24646 17822 2011 2111 2040 2228 2027 1005 2128 1999 1037 2537 1997 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] although some may call it a \" cuban cinema para ##dis ##o \" , the movie is closer to a how green was my valley , a memory film mourning for a lost innocence . the film smart ##ly avoids falling into a political trap of taking sides ( pro - castro ? anti - castro ? , focusing instead in the human frail ##ty of the characters and the importance of family . filled with good acting , in particular from mexican actress diana bra ##cho , who plays kei ##tel ' s wife . a masterpiece , filled with references to classic movies , from casablanca to chaplin ' s city lights . gael garcia bern ##al plays a small role which is critical [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] although some may call it a \" cuban cinema para ##dis ##o \" , the movie is closer to a how green was my valley , a memory film mourning for a lost innocence . the film smart ##ly avoids falling into a political trap of taking sides ( pro - castro ? anti - castro ? , focusing instead in the human frail ##ty of the characters and the importance of family . filled with good acting , in particular from mexican actress diana bra ##cho , who plays kei ##tel ' s wife . a masterpiece , filled with references to classic movies , from casablanca to chaplin ' s city lights . gael garcia bern ##al plays a small role which is critical [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2348 2070 2089 2655 2009 1037 1000 9642 5988 11498 10521 2080 1000 1010 1996 3185 2003 3553 2000 1037 2129 2665 2001 2026 3028 1010 1037 3638 2143 16236 2005 1037 2439 12660 1012 1996 2143 6047 2135 26777 4634 2046 1037 2576 8132 1997 2635 3903 1006 4013 1011 11794 1029 3424 1011 11794 1029 1010 7995 2612 1999 1996 2529 25737 3723 1997 1996 3494 1998 1996 5197 1997 2155 1012 3561 2007 2204 3772 1010 1999 3327 2013 4916 3883 8805 11655 9905 1010 2040 3248 26679 9834 1005 1055 2564 1012 1037 17743 1010 3561 2007 7604 2000 4438 5691 1010 2013 24592 2000 23331 1005 1055 2103 4597 1012 28151 7439 16595 2389 3248 1037 2235 2535 2029 2003 4187 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2348 2070 2089 2655 2009 1037 1000 9642 5988 11498 10521 2080 1000 1010 1996 3185 2003 3553 2000 1037 2129 2665 2001 2026 3028 1010 1037 3638 2143 16236 2005 1037 2439 12660 1012 1996 2143 6047 2135 26777 4634 2046 1037 2576 8132 1997 2635 3903 1006 4013 1011 11794 1029 3424 1011 11794 1029 1010 7995 2612 1999 1996 2529 25737 3723 1997 1996 3494 1998 1996 5197 1997 2155 1012 3561 2007 2204 3772 1010 1999 3327 2013 4916 3883 8805 11655 9905 1010 2040 3248 26679 9834 1005 1055 2564 1012 1037 17743 1010 3561 2007 7604 2000 4438 5691 1010 2013 24592 2000 23331 1005 1055 2103 4597 1012 28151 7439 16595 2389 3248 1037 2235 2535 2029 2003 4187 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] the crater lake monster is easily one of the most awful , amateur ##ish film i ' ve ever seen - ranking right up there with man ##os , the hands of fate in terms of poor acting , useless direction , and kindergarten - level production values . in this movie a silly - looking clay ##mation / stop - motion animated dinosaur wakes up after a meteor hits a lake in bum ##ble ##bu ##m , ca , and begins dining on the local hays ##eed ##s . in the thrilling climax , the creature , described by one local as \" a giant alligator with flip ##pers \" , drag ##s it ' s ponder ##ous bulk over the ground to chase its [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] the crater lake monster is easily one of the most awful , amateur ##ish film i ' ve ever seen - ranking right up there with man ##os , the hands of fate in terms of poor acting , useless direction , and kindergarten - level production values . in this movie a silly - looking clay ##mation / stop - motion animated dinosaur wakes up after a meteor hits a lake in bum ##ble ##bu ##m , ca , and begins dining on the local hays ##eed ##s . in the thrilling climax , the creature , described by one local as \" a giant alligator with flip ##pers \" , drag ##s it ' s ponder ##ous bulk over the ground to chase its [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1996 11351 2697 6071 2003 4089 2028 1997 1996 2087 9643 1010 5515 4509 2143 1045 1005 2310 2412 2464 1011 5464 2157 2039 2045 2007 2158 2891 1010 1996 2398 1997 6580 1999 3408 1997 3532 3772 1010 11809 3257 1010 1998 11793 1011 2504 2537 5300 1012 1999 2023 3185 1037 10021 1011 2559 5726 28649 1013 2644 1011 4367 6579 15799 17507 2039 2044 1037 23879 4978 1037 2697 1999 26352 3468 8569 2213 1010 6187 1010 1998 4269 7759 2006 1996 2334 29051 13089 2015 1012 1999 1996 26162 14463 1010 1996 6492 1010 2649 2011 2028 2334 2004 1000 1037 5016 28833 2007 11238 7347 1000 1010 8011 2015 2009 1005 1055 29211 3560 9625 2058 1996 2598 2000 5252 2049 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1996 11351 2697 6071 2003 4089 2028 1997 1996 2087 9643 1010 5515 4509 2143 1045 1005 2310 2412 2464 1011 5464 2157 2039 2045 2007 2158 2891 1010 1996 2398 1997 6580 1999 3408 1997 3532 3772 1010 11809 3257 1010 1998 11793 1011 2504 2537 5300 1012 1999 2023 3185 1037 10021 1011 2559 5726 28649 1013 2644 1011 4367 6579 15799 17507 2039 2044 1037 23879 4978 1037 2697 1999 26352 3468 8569 2213 1010 6187 1010 1998 4269 7759 2006 1996 2334 29051 13089 2015 1012 1999 1996 26162 14463 1010 1996 6492 1010 2649 2011 2028 2334 2004 1000 1037 5016 28833 2007 11238 7347 1000 1010 8011 2015 2009 1005 1055 29211 3560 9625 2058 1996 2598 2000 5252 2049 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i ' ve just lost 2 hours of my life watching this mind ##less plot . i could make a better movie with my cell ##phone camera . how do they manage to get actors to play in those movies ? ? porn movies have better scenarios and effects . . . i wish i had those 2 hours back . . . < br / > < br / > the only good thing about this movie is the cast . even though , their acting skills in this one could not lift this movie to pass ##able , the rest was just way too bad ! < br / > < br / > it ' s the type of movie that i ' [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i ' ve just lost 2 hours of my life watching this mind ##less plot . i could make a better movie with my cell ##phone camera . how do they manage to get actors to play in those movies ? ? porn movies have better scenarios and effects . . . i wish i had those 2 hours back . . . < br / > < br / > the only good thing about this movie is the cast . even though , their acting skills in this one could not lift this movie to pass ##able , the rest was just way too bad ! < br / > < br / > it ' s the type of movie that i ' [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 1005 2310 2074 2439 1016 2847 1997 2026 2166 3666 2023 2568 3238 5436 1012 1045 2071 2191 1037 2488 3185 2007 2026 3526 9864 4950 1012 2129 2079 2027 6133 2000 2131 5889 2000 2377 1999 2216 5691 1029 1029 22555 5691 2031 2488 16820 1998 3896 1012 1012 1012 1045 4299 1045 2018 2216 1016 2847 2067 1012 1012 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2069 2204 2518 2055 2023 3185 2003 1996 3459 1012 2130 2295 1010 2037 3772 4813 1999 2023 2028 2071 2025 6336 2023 3185 2000 3413 3085 1010 1996 2717 2001 2074 2126 2205 2919 999 1026 7987 1013 1028 1026 7987 1013 1028 2009 1005 1055 1996 2828 1997 3185 2008 1045 1005 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 1005 2310 2074 2439 1016 2847 1997 2026 2166 3666 2023 2568 3238 5436 1012 1045 2071 2191 1037 2488 3185 2007 2026 3526 9864 4950 1012 2129 2079 2027 6133 2000 2131 5889 2000 2377 1999 2216 5691 1029 1029 22555 5691 2031 2488 16820 1998 3896 1012 1012 1012 1045 4299 1045 2018 2216 1016 2847 2067 1012 1012 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2069 2204 2518 2055 2023 3185 2003 1996 3459 1012 2130 2295 1010 2037 3772 4813 1999 2023 2028 2071 2025 6336 2023 3185 2000 3413 3085 1010 1996 2717 2001 2074 2126 2205 2919 999 1026 7987 1013 1028 1026 7987 1013 1028 2009 1005 1055 1996 2828 1997 3185 2008 1045 1005 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] leo ##s cara ##x has made 3 great movies : boys meet girls , ma ##u ##va ##is sang , les ama ##nts du pont ne ##uf . in fact those films were not that great but it has the violence of youth , the beauty of juvenile wilderness . cara ##x in these three movies was well aware of what cinema was , but he tried to make his own vision of the art , without thinking about about all he have seen , but using it and melting it into his times . pol ##a x is a very different movie because cara ##x made les ama ##nts du pont ne ##uf , a mons ##tr ##uo ##sity of 20 millions dollars , a [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] leo ##s cara ##x has made 3 great movies : boys meet girls , ma ##u ##va ##is sang , les ama ##nts du pont ne ##uf . in fact those films were not that great but it has the violence of youth , the beauty of juvenile wilderness . cara ##x in these three movies was well aware of what cinema was , but he tried to make his own vision of the art , without thinking about about all he have seen , but using it and melting it into his times . pol ##a x is a very different movie because cara ##x made les ama ##nts du pont ne ##uf , a mons ##tr ##uo ##sity of 20 millions dollars , a [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 6688 2015 14418 2595 2038 2081 1017 2307 5691 1024 3337 3113 3057 1010 5003 2226 3567 2483 6369 1010 4649 25933 7666 4241 21179 11265 16093 1012 1999 2755 2216 3152 2020 2025 2008 2307 2021 2009 2038 1996 4808 1997 3360 1010 1996 5053 1997 11799 9917 1012 14418 2595 1999 2122 2093 5691 2001 2092 5204 1997 2054 5988 2001 1010 2021 2002 2699 2000 2191 2010 2219 4432 1997 1996 2396 1010 2302 3241 2055 2055 2035 2002 2031 2464 1010 2021 2478 2009 1998 13721 2009 2046 2010 2335 1012 14955 2050 1060 2003 1037 2200 2367 3185 2138 14418 2595 2081 4649 25933 7666 4241 21179 11265 16093 1010 1037 29408 16344 19098 17759 1997 2322 8817 6363 1010 1037 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 6688 2015 14418 2595 2038 2081 1017 2307 5691 1024 3337 3113 3057 1010 5003 2226 3567 2483 6369 1010 4649 25933 7666 4241 21179 11265 16093 1012 1999 2755 2216 3152 2020 2025 2008 2307 2021 2009 2038 1996 4808 1997 3360 1010 1996 5053 1997 11799 9917 1012 14418 2595 1999 2122 2093 5691 2001 2092 5204 1997 2054 5988 2001 1010 2021 2002 2699 2000 2191 2010 2219 4432 1997 1996 2396 1010 2302 3241 2055 2055 2035 2002 2031 2464 1010 2021 2478 2009 1998 13721 2009 2046 2010 2335 1012 14955 2050 1060 2003 1037 2200 2367 3185 2138 14418 2595 2081 4649 25933 7666 4241 21179 11265 16093 1010 1037 29408 16344 19098 17759 1997 2322 8817 6363 1010 1037 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this was a pretty dull movie , actually . i think the problem with a french horror film , is that the french must be easy to scare or something , because this movie wasn ' t just that frightening . the special effects with the mummy ' s ghost looked like they didn ' t even belong in the film , as though someone put them in during post - production to spice them up , because the actors barely react to them . < br / > < br / > the plot just kind of mean ##ders , which is the opposite of real storytelling . i guess this was based on a french tv series , where they had to di ##sti [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this was a pretty dull movie , actually . i think the problem with a french horror film , is that the french must be easy to scare or something , because this movie wasn ' t just that frightening . the special effects with the mummy ' s ghost looked like they didn ' t even belong in the film , as though someone put them in during post - production to spice them up , because the actors barely react to them . < br / > < br / > the plot just kind of mean ##ders , which is the opposite of real storytelling . i guess this was based on a french tv series , where they had to di ##sti [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2001 1037 3492 10634 3185 1010 2941 1012 1045 2228 1996 3291 2007 1037 2413 5469 2143 1010 2003 2008 1996 2413 2442 2022 3733 2000 12665 2030 2242 1010 2138 2023 3185 2347 1005 1056 2074 2008 17115 1012 1996 2569 3896 2007 1996 22788 1005 1055 5745 2246 2066 2027 2134 1005 1056 2130 7141 1999 1996 2143 1010 2004 2295 2619 2404 2068 1999 2076 2695 1011 2537 2000 17688 2068 2039 1010 2138 1996 5889 4510 10509 2000 2068 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 5436 2074 2785 1997 2812 13375 1010 2029 2003 1996 4500 1997 2613 20957 1012 1045 3984 2023 2001 2241 2006 1037 2413 2694 2186 1010 2073 2027 2018 2000 4487 16643 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2001 1037 3492 10634 3185 1010 2941 1012 1045 2228 1996 3291 2007 1037 2413 5469 2143 1010 2003 2008 1996 2413 2442 2022 3733 2000 12665 2030 2242 1010 2138 2023 3185 2347 1005 1056 2074 2008 17115 1012 1996 2569 3896 2007 1996 22788 1005 1055 5745 2246 2066 2027 2134 1005 1056 2130 7141 1999 1996 2143 1010 2004 2295 2619 2404 2068 1999 2076 2695 1011 2537 2000 17688 2068 2039 1010 2138 1996 5889 4510 10509 2000 2068 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 5436 2074 2785 1997 2812 13375 1010 2029 2003 1996 4500 1997 2613 20957 1012 1045 3984 2023 2001 2241 2006 1037 2413 2694 2186 1010 2073 2027 2018 2000 4487 16643 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this movie is just plain dumb . don ' t bother watching it ; believe me , you ' re better off . < br / > < br / > long and short of the plot : a defense attorney represents a man who murdered his son and other children . in defending him , she comes across a wooden doll of pin ##no ##chio . she takes the doll home . pin ##no ##chio is possessed and begins to start killing people . < br / > < br / > this movie moves very slowly only to have such a weak ending . the plot is very bad and the dennis michael ten ##ney ' s musical score is pit ##iful . the [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this movie is just plain dumb . don ' t bother watching it ; believe me , you ' re better off . < br / > < br / > long and short of the plot : a defense attorney represents a man who murdered his son and other children . in defending him , she comes across a wooden doll of pin ##no ##chio . she takes the doll home . pin ##no ##chio is possessed and begins to start killing people . < br / > < br / > this movie moves very slowly only to have such a weak ending . the plot is very bad and the dennis michael ten ##ney ' s musical score is pit ##iful . the [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 3185 2003 2074 5810 12873 1012 2123 1005 1056 8572 3666 2009 1025 2903 2033 1010 2017 1005 2128 2488 2125 1012 1026 7987 1013 1028 1026 7987 1013 1028 2146 1998 2460 1997 1996 5436 1024 1037 3639 4905 5836 1037 2158 2040 7129 2010 2365 1998 2060 2336 1012 1999 6984 2032 1010 2016 3310 2408 1037 4799 10658 1997 9231 3630 23584 1012 2016 3138 1996 10658 2188 1012 9231 3630 23584 2003 8679 1998 4269 2000 2707 4288 2111 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 3185 5829 2200 3254 2069 2000 2031 2107 1037 5410 4566 1012 1996 5436 2003 2200 2919 1998 1996 6877 2745 2702 5420 1005 1055 3315 3556 2003 6770 18424 1012 1996 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 3185 2003 2074 5810 12873 1012 2123 1005 1056 8572 3666 2009 1025 2903 2033 1010 2017 1005 2128 2488 2125 1012 1026 7987 1013 1028 1026 7987 1013 1028 2146 1998 2460 1997 1996 5436 1024 1037 3639 4905 5836 1037 2158 2040 7129 2010 2365 1998 2060 2336 1012 1999 6984 2032 1010 2016 3310 2408 1037 4799 10658 1997 9231 3630 23584 1012 2016 3138 1996 10658 2188 1012 9231 3630 23584 2003 8679 1998 4269 2000 2707 4288 2111 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 3185 5829 2200 3254 2069 2000 2031 2107 1037 5410 4566 1012 1996 5436 2003 2200 2919 1998 1996 6877 2745 2702 5420 1005 1055 3315 3556 2003 6770 18424 1012 1996 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] again such kind of zero - budget digital - video cam trash . and again i fell into this trap cu ##z the title had \" zombie \" in it ( german title : zombie attack ! ) the story : on halloween some people visit the \" museum of the dead \" , it ' s a trap , a crazy doctor wants to kill the people , everything connected to some aztec - cult . so they fight against some zombies in there . < br / > < br / > ultra cheap scenery : some corridors with black tape on it . a few dil ##etta ##nti ##sh drawings and a few skulls as you can find them in every fun - [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] again such kind of zero - budget digital - video cam trash . and again i fell into this trap cu ##z the title had \" zombie \" in it ( german title : zombie attack ! ) the story : on halloween some people visit the \" museum of the dead \" , it ' s a trap , a crazy doctor wants to kill the people , everything connected to some aztec - cult . so they fight against some zombies in there . < br / > < br / > ultra cheap scenery : some corridors with black tape on it . a few dil ##etta ##nti ##sh drawings and a few skulls as you can find them in every fun - [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2153 2107 2785 1997 5717 1011 5166 3617 1011 2678 11503 11669 1012 1998 2153 1045 3062 2046 2023 8132 12731 2480 1996 2516 2018 1000 11798 1000 1999 2009 1006 2446 2516 1024 11798 2886 999 1007 1996 2466 1024 2006 14414 2070 2111 3942 1996 1000 2688 1997 1996 2757 1000 1010 2009 1005 1055 1037 8132 1010 1037 4689 3460 4122 2000 3102 1996 2111 1010 2673 4198 2000 2070 25245 1011 8754 1012 2061 2027 2954 2114 2070 14106 1999 2045 1012 1026 7987 1013 1028 1026 7987 1013 1028 11087 10036 17363 1024 2070 17506 2007 2304 6823 2006 2009 1012 1037 2261 29454 16549 16778 4095 9254 1998 1037 2261 21542 2004 2017 2064 2424 2068 1999 2296 4569 1011 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2153 2107 2785 1997 5717 1011 5166 3617 1011 2678 11503 11669 1012 1998 2153 1045 3062 2046 2023 8132 12731 2480 1996 2516 2018 1000 11798 1000 1999 2009 1006 2446 2516 1024 11798 2886 999 1007 1996 2466 1024 2006 14414 2070 2111 3942 1996 1000 2688 1997 1996 2757 1000 1010 2009 1005 1055 1037 8132 1010 1037 4689 3460 4122 2000 3102 1996 2111 1010 2673 4198 2000 2070 25245 1011 8754 1012 2061 2027 2954 2114 2070 14106 1999 2045 1012 1026 7987 1013 1028 1026 7987 1013 1028 11087 10036 17363 1024 2070 17506 2007 2304 6823 2006 2009 1012 1037 2261 29454 16549 16778 4095 9254 1998 1037 2261 21542 2004 2017 2064 2424 2068 1999 2296 4569 1011 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] if you want to really terri ##fy people , choose the devil as your subject . after all , a good deal of the population believe that he is real . therefore you are plug ##ging into a whole meat ##y swat ##he of pre - existing religious and mythological imagery . and bound to cause quite a few nightmares in your young audience . < br / > < br / > this episode had all the appearance of a hollywood blockbuster . in fact , having finished watching it i flicked over to another channel which was playing a recent bond film and quite frankly couldn ' t split a hair between the differences in sf ##x . with a minimal cast , restricted [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] if you want to really terri ##fy people , choose the devil as your subject . after all , a good deal of the population believe that he is real . therefore you are plug ##ging into a whole meat ##y swat ##he of pre - existing religious and mythological imagery . and bound to cause quite a few nightmares in your young audience . < br / > < br / > this episode had all the appearance of a hollywood blockbuster . in fact , having finished watching it i flicked over to another channel which was playing a recent bond film and quite frankly couldn ' t split a hair between the differences in sf ##x . with a minimal cast , restricted [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2065 2017 2215 2000 2428 26568 12031 2111 1010 5454 1996 6548 2004 2115 3395 1012 2044 2035 1010 1037 2204 3066 1997 1996 2313 2903 2008 2002 2003 2613 1012 3568 2017 2024 13354 4726 2046 1037 2878 6240 2100 25414 5369 1997 3653 1011 4493 3412 1998 21637 13425 1012 1998 5391 2000 3426 3243 1037 2261 15446 1999 2115 2402 4378 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 2792 2018 2035 1996 3311 1997 1037 5365 27858 1012 1999 2755 1010 2383 2736 3666 2009 1045 12777 2058 2000 2178 3149 2029 2001 2652 1037 3522 5416 2143 1998 3243 19597 2481 1005 1056 3975 1037 2606 2090 1996 5966 1999 16420 2595 1012 2007 1037 10124 3459 1010 7775 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2065 2017 2215 2000 2428 26568 12031 2111 1010 5454 1996 6548 2004 2115 3395 1012 2044 2035 1010 1037 2204 3066 1997 1996 2313 2903 2008 2002 2003 2613 1012 3568 2017 2024 13354 4726 2046 1037 2878 6240 2100 25414 5369 1997 3653 1011 4493 3412 1998 21637 13425 1012 1998 5391 2000 3426 3243 1037 2261 15446 1999 2115 2402 4378 1012 1026 7987 1013 1028 1026 7987 1013 1028 2023 2792 2018 2035 1996 3311 1997 1037 5365 27858 1012 1999 2755 1010 2383 2736 3666 2009 1045 12777 2058 2000 2178 3149 2029 2001 2652 1037 3522 5416 2143 1998 3243 19597 2481 1005 1056 3975 1037 2606 2090 1996 5966 1999 16420 2595 1012 2007 1037 10124 3459 1010 7775 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "    bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for sentiment data.\n",
    "    output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)\n",
    "\n",
    "    \n",
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "\n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "\n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "            train_op = bert.optimization.create_optimizer(\n",
    "            loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "            # Calculate evaluation metrics. \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.contrib.metrics.f1_score(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                auc = tf.metrics.auc(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                recall = tf.metrics.recall(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                precision = tf.metrics.precision(\n",
    "                    label_ids,\n",
    "                    predicted_labels) \n",
    "                true_pos = tf.metrics.true_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)   \n",
    "                false_pos = tf.metrics.false_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)  \n",
    "                false_neg = tf.metrics.false_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"f1_score\": f1_score,\n",
    "                    \"auc\": auc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg\n",
    "                }\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                    loss=loss,\n",
    "                    train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                    loss=loss,\n",
    "                    eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(\n",
    "                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "            predictions = {\n",
    "            'probabilities': log_probs,\n",
    "            'labels': predicted_labels\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Return the actual model function in the closure\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Ready for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "# SAVE_CHECKPOINTS_STEPS = 500\n",
    "# SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "# # Specify outpit directory and number of checkpoint steps to save\n",
    "# run_config = tf.estimator.RunConfig(\n",
    "#     model_dir=OUTPUT_DIR,\n",
    "#     save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "#     save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/bv/tqd8yzws73j5hszbpwd8vkk80000gn/T/tmprws1vntv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/bv/tqd8yzws73j5hszbpwd8vkk80000gn/T/tmprws1vntv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/bv/tqd8yzws73j5hszbpwd8vkk80000gn/T/tmprws1vntv', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13362b588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/bv/tqd8yzws73j5hszbpwd8vkk80000gn/T/tmprws1vntv', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x13362b588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  #config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Your Own Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences):\n",
    "    labels = [\"Negative\", \"Positive\"]\n",
    "    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\"\n",
    "]\n",
    "\n",
    "predictions = getPrediction(pred_sentences)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "BERT_tutorial_env",
   "language": "python",
   "name": "bert_tutorial_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
